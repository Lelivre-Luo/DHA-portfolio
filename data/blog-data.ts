// This file is auto-generated. Do not edit manually.
// Run 'pnpm run build:content' to regenerate.

export type BlogPost = {
  id: number
  title: string
  excerpt: string
  date: string
  readTime: string
  slug: string
  tags: string[]
  image?: string | null
  content?: string
}

export const allBlogPosts: BlogPost[] = [
  {
    "id": 1,
    "title": "GraphQL vs REST in Python Backend Applications",
    "excerpt": "An in-depth comparison of GraphQL and REST API approaches in Python backend applications, with real-world examples and performance considerations.",
    "date": "January 22, 2023",
    "readTime": "8 min read",
    "image": null,
    "slug": "graphql-vs-rest-python",
    "tags": [
      "GraphQL",
      "REST API",
      "Python",
      "Backend"
    ],
    "content": "<h1>GraphQL vs REST in Python Backend Applications</h1>\n<p>When building modern web applications, choosing between GraphQL and REST APIs is a crucial decision that affects your application's architecture, performance, and developer experience. This comprehensive guide compares both approaches in the context of Python backend development.</p>\n<h2>Understanding REST APIs</h2>\n<p>REST (Representational State Transfer) is an architectural style that uses HTTP methods to perform operations on resources. It's been the standard for web APIs for many years.</p>\n<h3>REST Characteristics</h3>\n<ul>\n<li><strong>Resource-based</strong>: URLs represent resources</li>\n<li><strong>HTTP methods</strong>: GET, POST, PUT, DELETE for different operations</li>\n<li><strong>Stateless</strong>: Each request contains all necessary information</li>\n<li><strong>Cacheable</strong>: Responses can be cached</li>\n<li><strong>Uniform interface</strong>: Consistent API design</li>\n</ul>\n<h3>Python REST Implementation with FastAPI</h3>\n<pre><code class=\"language-python\">from fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nfrom typing import List, Optional\n\napp = FastAPI()\n\nclass User(BaseModel):\n    id: int\n    name: str\n    email: str\n    age: Optional[int] = None\n\nclass Post(BaseModel):\n    id: int\n    title: str\n    content: str\n    author_id: int\n\n# REST endpoints\n@app.get(\"/users\", response_model=List[User])\nasync def get_users(skip: int = 0, limit: int = 10):\n    # Return list of users\n    pass\n\n@app.get(\"/users/{user_id}\", response_model=User)\nasync def get_user(user_id: int):\n    # Return specific user\n    pass\n\n@app.get(\"/users/{user_id}/posts\", response_model=List[Post])\nasync def get_user_posts(user_id: int):\n    # Return posts for specific user\n    pass\n\n@app.post(\"/users\", response_model=User)\nasync def create_user(user: User):\n    # Create new user\n    pass\n</code></pre>\n<h2>Understanding GraphQL</h2>\n<p>GraphQL is a query language and runtime for APIs that allows clients to request exactly the data they need. It was developed by Facebook and provides a more flexible approach to data fetching.</p>\n<h3>GraphQL Characteristics</h3>\n<ul>\n<li><strong>Single endpoint</strong>: All queries go to one endpoint</li>\n<li><strong>Client-specified queries</strong>: Clients define what data they want</li>\n<li><strong>Strongly typed</strong>: Schema defines the API structure</li>\n<li><strong>Real-time subscriptions</strong>: Built-in support for real-time updates</li>\n<li><strong>Introspection</strong>: Self-documenting API</li>\n</ul>\n<h3>Python GraphQL Implementation with Strawberry</h3>\n<pre><code class=\"language-python\">import strawberry\nfrom strawberry.fastapi import GraphQLRouter\nfrom typing import List, Optional\n\n@strawberry.type\nclass User:\n    id: int\n    name: str\n    email: str\n    age: Optional[int] = None\n    posts: List['Post'] = strawberry.field(default_factory=list)\n\n@strawberry.type\nclass Post:\n    id: int\n    title: str\n    content: str\n    author: User\n\n@strawberry.type\nclass Query:\n    @strawberry.field\n    def users(self, skip: int = 0, limit: int = 10) -> List[User]:\n        # Return list of users\n        pass\n    \n    @strawberry.field\n    def user(self, user_id: int) -> Optional[User]:\n        # Return specific user\n        pass\n\n@strawberry.type\nclass Mutation:\n    @strawberry.mutation\n    def create_user(self, name: str, email: str, age: Optional[int] = None) -> User:\n        # Create new user\n        pass\n\nschema = strawberry.Schema(query=Query, mutation=Mutation)\ngraphql_app = GraphQLRouter(schema)\napp.include_router(graphql_app, prefix=\"/graphql\")\n</code></pre>\n<h2>Detailed Comparison</h2>\n<h3>1. Data Fetching</h3>\n<p><strong>REST Approach:</strong></p>\n<pre><code class=\"language-python\"># Multiple requests needed\nGET /users/1\nGET /users/1/posts\nGET /users/1/posts/1/comments\n</code></pre>\n<p><strong>GraphQL Approach:</strong></p>\n<pre><code class=\"language-graphql\">query {\n  user(id: 1) {\n    name\n    email\n    posts {\n      title\n      content\n      comments {\n        text\n        author {\n          name\n        }\n      }\n    }\n  }\n}\n</code></pre>\n<h3>2. Over-fetching and Under-fetching</h3>\n<p><strong>REST Problems:</strong></p>\n<ul>\n<li><strong>Over-fetching</strong>: Getting more data than needed</li>\n<li><strong>Under-fetching</strong>: Not getting enough data, requiring multiple requests</li>\n</ul>\n<p><strong>GraphQL Solution:</strong></p>\n<ul>\n<li>Clients request exactly what they need</li>\n<li>Single request for complex data structures</li>\n<li>Reduced bandwidth usage</li>\n</ul>\n<h3>3. API Evolution</h3>\n<p><strong>REST:</strong></p>\n<pre><code class=\"language-python\"># Versioning required for breaking changes\n@app.get(\"/api/v1/users\")\nasync def get_users_v1():\n    pass\n\n@app.get(\"/api/v2/users\")\nasync def get_users_v2():\n    pass\n</code></pre>\n<p><strong>GraphQL:</strong></p>\n<pre><code class=\"language-python\"># Add new fields without breaking existing clients\n@strawberry.type\nclass User:\n    id: int\n    name: str\n    email: str\n    # New field added\n    phone: Optional[str] = None\n</code></pre>\n<h2>Performance Considerations</h2>\n<h3>REST Performance</h3>\n<p><strong>Advantages:</strong></p>\n<ul>\n<li>HTTP caching works well</li>\n<li>Simple to implement</li>\n<li>CDN-friendly</li>\n<li>Well-understood by developers</li>\n</ul>\n<p><strong>Disadvantages:</strong></p>\n<ul>\n<li>Multiple requests for complex data</li>\n<li>Over-fetching common</li>\n<li>N+1 query problems</li>\n</ul>\n<h3>GraphQL Performance</h3>\n<p><strong>Advantages:</strong></p>\n<ul>\n<li>Single request for complex data</li>\n<li>Reduced bandwidth</li>\n<li>Client controls data shape</li>\n<li>Real-time subscriptions</li>\n</ul>\n<p><strong>Disadvantages:</strong></p>\n<ul>\n<li>Complex caching strategies</li>\n<li>Potential for expensive queries</li>\n<li>Learning curve for developers</li>\n</ul>\n<h2>Caching Strategies</h2>\n<h3>REST Caching</h3>\n<pre><code class=\"language-python\">from fastapi import FastAPI, Request\nfrom fastapi.responses import Response\nimport redis\nimport json\n\nredis_client = redis.Redis(host='localhost', port=6379, db=0)\n\n@app.get(\"/users/{user_id}\")\nasync def get_user(user_id: int):\n    cache_key = f\"user:{user_id}\"\n    cached = redis_client.get(cache_key)\n    \n    if cached:\n        return json.loads(cached)\n    \n    user = await fetch_user_from_db(user_id)\n    redis_client.setex(cache_key, 300, json.dumps(user.dict()))\n    return user\n</code></pre>\n<h3>GraphQL Caching</h3>\n<pre><code class=\"language-python\">import strawberry\nfrom strawberry.extensions import QueryDepthLimiter, QueryComplexityLimiter\n\n# Add query complexity analysis\n@strawberry.type\nclass Query:\n    @strawberry.field\n    def users(self) -> List[User]:\n        # Implement field-level caching\n        pass\n\n# Configure extensions\nschema = strawberry.Schema(\n    query=Query,\n    extensions=[\n        QueryDepthLimiter(max_depth=10),\n        QueryComplexityLimiter(max_complexity=1000)\n    ]\n)\n</code></pre>\n<h2>Error Handling</h2>\n<h3>REST Error Handling</h3>\n<pre><code class=\"language-python\">from fastapi import HTTPException\n\n@app.get(\"/users/{user_id}\")\nasync def get_user(user_id: int):\n    user = await fetch_user_from_db(user_id)\n    if not user:\n        raise HTTPException(status_code=404, detail=\"User not found\")\n    return user\n</code></pre>\n<h3>GraphQL Error Handling</h3>\n<pre><code class=\"language-python\">import strawberry\nfrom strawberry.types import Info\n\n@strawberry.type\nclass UserNotFound:\n    message: str = \"User not found\"\n\n@strawberry.type\nclass User:\n    id: int\n    name: str\n\n@strawberry.type\nclass Query:\n    @strawberry.field\n    def user(self, user_id: int) -> strawberry.union(\"UserResult\", User, UserNotFound):\n        user = fetch_user_from_db(user_id)\n        if not user:\n            return UserNotFound()\n        return user\n</code></pre>\n<h2>When to Choose REST</h2>\n<p><strong>Choose REST when:</strong></p>\n<ul>\n<li>Building simple CRUD applications</li>\n<li>Need HTTP caching</li>\n<li>Working with mobile apps (better caching support)</li>\n<li>Team is more familiar with REST</li>\n<li>Building public APIs</li>\n<li>Need simple integration with CDNs</li>\n</ul>\n<h2>When to Choose GraphQL</h2>\n<p><strong>Choose GraphQL when:</strong></p>\n<ul>\n<li>Building complex applications with varied data needs</li>\n<li>Need real-time subscriptions</li>\n<li>Want to reduce over-fetching</li>\n<li>Building internal APIs</li>\n<li>Need strong typing and introspection</li>\n<li>Working with modern frontend frameworks</li>\n</ul>\n<h2>Hybrid Approach</h2>\n<p>You can also use both approaches in the same application:</p>\n<pre><code class=\"language-python\">from fastapi import FastAPI\nfrom strawberry.fastapi import GraphQLRouter\n\napp = FastAPI()\n\n# REST endpoints for simple operations\n@app.get(\"/users/{user_id}\")\nasync def get_user_rest(user_id: int):\n    pass\n\n# GraphQL for complex queries\n@strawberry.type\nclass Query:\n    @strawberry.field\n    def user_with_posts_and_comments(self, user_id: int):\n        pass\n\nschema = strawberry.Schema(query=Query)\ngraphql_app = GraphQLRouter(schema)\napp.include_router(graphql_app, prefix=\"/graphql\")\n</code></pre>\n<h2>Conclusion</h2>\n<p>Both REST and GraphQL have their place in modern Python backend development:</p>\n<ul>\n<li><strong>REST</strong> is simpler, more widely understood, and works well for straightforward APIs</li>\n<li><strong>GraphQL</strong> provides more flexibility and efficiency for complex applications</li>\n</ul>\n<p>The choice depends on your specific requirements, team expertise, and application complexity. Consider starting with REST for simplicity and migrating to GraphQL as your needs grow more complex.</p>\n<p>Remember that the best API is one that serves your users' needs effectively while being maintainable and performant for your development team.</p>\n"
  },
  {
    "id": 2,
    "title": "Optimizing FastAPI Performance for High-Traffic Applications",
    "excerpt": "Learn advanced techniques for optimizing FastAPI applications to handle high traffic loads, including async/await patterns, database optimizations, and caching strategies.",
    "date": "March 15, 2023",
    "readTime": "10 min read",
    "image": null,
    "slug": "optimizing-fastapi-performance",
    "tags": [
      "FastAPI",
      "Python",
      "Performance",
      "Backend"
    ],
    "content": "<h1>Optimizing FastAPI Performance for High-Traffic Applications</h1>\n<p>FastAPI has become one of the most popular Python web frameworks for building high-performance APIs. However, as your application scales and handles more traffic, you'll need to implement specific optimization techniques to maintain excellent performance.</p>\n<h2>Understanding FastAPI's Performance Characteristics</h2>\n<p>FastAPI is built on top of Starlette and Pydantic, which provides several performance advantages:</p>\n<ul>\n<li><strong>Automatic async/await support</strong>: Built-in support for Python's async/await syntax</li>\n<li><strong>Type validation with Pydantic</strong>: Fast request/response validation</li>\n<li><strong>Automatic OpenAPI documentation</strong>: No performance overhead for documentation generation</li>\n<li><strong>Dependency injection system</strong>: Efficient dependency management</li>\n</ul>\n<h2>Database Optimization Strategies</h2>\n<h3>1. Connection Pooling</h3>\n<pre><code class=\"language-python\">from sqlalchemy import create_engine\nfrom sqlalchemy.pool import QueuePool\n\nengine = create_engine(\n    \"postgresql://user:password@localhost/db\",\n    poolclass=QueuePool,\n    pool_size=20,\n    max_overflow=30,\n    pool_pre_ping=True\n)\n</code></pre>\n<h3>2. Query Optimization</h3>\n<pre><code class=\"language-python\">from sqlalchemy.orm import selectinload, joinedload\n\n# Use selectinload for one-to-many relationships\nusers = await session.execute(\n    select(User).options(selectinload(User.posts))\n)\n\n# Use joinedload for many-to-one relationships\nposts = await session.execute(\n    select(Post).options(joinedload(Post.author))\n)\n</code></pre>\n<h2>Caching Implementation</h2>\n<h3>Redis Caching</h3>\n<pre><code class=\"language-python\">import redis\nimport json\nfrom functools import wraps\n\nredis_client = redis.Redis(host='localhost', port=6379, db=0)\n\ndef cache_result(expiration: int = 300):\n    def decorator(func):\n        @wraps(func)\n        async def wrapper(*args, **kwargs):\n            cache_key = f\"{func.__name__}:{hash(str(args) + str(kwargs))}\"\n            cached = redis_client.get(cache_key)\n            \n            if cached:\n                return json.loads(cached)\n            \n            result = await func(*args, **kwargs)\n            redis_client.setex(cache_key, expiration, json.dumps(result))\n            return result\n        return wrapper\n    return decorator\n</code></pre>\n<h2>Async Best Practices</h2>\n<h3>1. Proper Async Function Usage</h3>\n<pre><code class=\"language-python\"># Good: Use async for I/O operations\nasync def get_user_data(user_id: int):\n    async with get_db_session() as session:\n        result = await session.execute(select(User).where(User.id == user_id))\n        return result.scalar_one_or_none()\n\n# Avoid: Don't use async for CPU-bound operations\nasync def calculate_fibonacci(n: int):  # This is wrong\n    if n &#x3C;= 1:\n        return n\n    return await calculate_fibonacci(n-1) + await calculate_fibonacci(n-2)\n</code></pre>\n<h3>2. Background Tasks</h3>\n<pre><code class=\"language-python\">from fastapi import BackgroundTasks\n\n@app.post(\"/send-email\")\nasync def send_email(\n    email: EmailSchema,\n    background_tasks: BackgroundTasks\n):\n    background_tasks.add_task(send_email_task, email)\n    return {\"message\": \"Email will be sent in the background\"}\n</code></pre>\n<h2>Monitoring and Profiling</h2>\n<h3>1. Request Timing Middleware</h3>\n<pre><code class=\"language-python\">import time\nfrom fastapi import Request\n\n@app.middleware(\"http\")\nasync def add_process_time_header(request: Request, call_next):\n    start_time = time.time()\n    response = await call_next(request)\n    process_time = time.time() - start_time\n    response.headers[\"X-Process-Time\"] = str(process_time)\n    return response\n</code></pre>\n<h3>2. Database Query Monitoring</h3>\n<pre><code class=\"language-python\">from sqlalchemy import event\nfrom sqlalchemy.engine import Engine\nimport logging\n\nlogging.basicConfig()\nlogger = logging.getLogger(\"sqlalchemy.engine\")\nlogger.setLevel(logging.INFO)\n\n@event.listens_for(Engine, \"before_cursor_execute\")\ndef receive_before_cursor_execute(conn, cursor, statement, parameters, context, executemany):\n    context._query_start_time = time.time()\n\n@event.listens_for(Engine, \"after_cursor_execute\")\ndef receive_after_cursor_execute(conn, cursor, statement, parameters, context, executemany):\n    total = time.time() - context._query_start_time\n    logger.info(\"Total time: %f\", total)\n</code></pre>\n<h2>Conclusion</h2>\n<p>Optimizing FastAPI performance requires a multi-faceted approach:</p>\n<ol>\n<li><strong>Database optimization</strong>: Proper connection pooling and query optimization</li>\n<li><strong>Caching strategies</strong>: Implement Redis or in-memory caching</li>\n<li><strong>Async best practices</strong>: Use async/await correctly for I/O operations</li>\n<li><strong>Background tasks</strong>: Offload heavy operations</li>\n<li><strong>Monitoring</strong>: Track performance metrics and identify bottlenecks</li>\n</ol>\n<p>By implementing these techniques, you can build FastAPI applications that handle high traffic loads while maintaining excellent response times and user experience.</p>\n"
  },
  {
    "id": 3,
    "title": "Building Scalable Python Microservices with FastAPI and RabbitMQ",
    "excerpt": "A comprehensive guide to designing and implementing a scalable microservices architecture using Python, FastAPI, and message queues for reliable communication.",
    "date": "February 8, 2023",
    "readTime": "12 min read",
    "image": null,
    "slug": "scalable-python-microservices",
    "tags": [
      "Microservices",
      "Python",
      "FastAPI",
      "RabbitMQ"
    ],
    "content": "<h1>Building Scalable Python Microservices with FastAPI and RabbitMQ</h1>\n<p>Microservices architecture has become the go-to approach for building large-scale, maintainable applications. This guide will walk you through creating a robust microservices system using Python, FastAPI, and RabbitMQ.</p>\n<h2>Understanding Microservices Architecture</h2>\n<p>Microservices are small, independent services that communicate over well-defined APIs. Each service is responsible for a specific business capability and can be developed, deployed, and scaled independently.</p>\n<h3>Key Benefits</h3>\n<ul>\n<li><strong>Scalability</strong>: Scale individual services based on demand</li>\n<li><strong>Technology diversity</strong>: Use different technologies for different services</li>\n<li><strong>Fault isolation</strong>: Failure in one service doesn't bring down the entire system</li>\n<li><strong>Team autonomy</strong>: Different teams can work on different services</li>\n</ul>\n<h2>Service Communication Patterns</h2>\n<h3>1. Synchronous Communication (HTTP/REST)</h3>\n<pre><code class=\"language-python\"># Service A calling Service B\nimport httpx\n\nasync def get_user_orders(user_id: int):\n    async with httpx.AsyncClient() as client:\n        response = await client.get(f\"http://user-service:8000/users/{user_id}\")\n        user = response.json()\n        \n        response = await client.get(f\"http://order-service:8000/orders?user_id={user_id}\")\n        orders = response.json()\n        \n        return {\"user\": user, \"orders\": orders}\n</code></pre>\n<h3>2. Asynchronous Communication (Message Queues)</h3>\n<pre><code class=\"language-python\"># Publisher\nimport pika\nimport json\n\ndef publish_order_created(order_data):\n    connection = pika.BlockingConnection(pika.ConnectionParameters('localhost'))\n    channel = connection.channel()\n    \n    channel.queue_declare(queue='order_created')\n    channel.basic_publish(\n        exchange='',\n        routing_key='order_created',\n        body=json.dumps(order_data)\n    )\n    connection.close()\n\n# Consumer\ndef consume_order_created():\n    connection = pika.BlockingConnection(pika.ConnectionParameters('localhost'))\n    channel = connection.channel()\n    \n    channel.queue_declare(queue='order_created')\n    \n    def callback(ch, method, properties, body):\n        order_data = json.loads(body)\n        # Process the order\n        print(f\"Processing order: {order_data}\")\n    \n    channel.basic_consume(queue='order_created', on_message_callback=callback)\n    channel.start_consuming()\n</code></pre>\n<h2>Service Discovery and Load Balancing</h2>\n<h3>Using Consul for Service Discovery</h3>\n<pre><code class=\"language-python\">import consul\nimport random\n\nclass ServiceDiscovery:\n    def __init__(self):\n        self.consul = consul.Consul()\n    \n    def register_service(self, service_name, service_id, address, port):\n        self.consul.agent.service.register(\n            name=service_name,\n            service_id=service_id,\n            address=address,\n            port=port,\n            check=consul.Check.http(f\"http://{address}:{port}/health\", \"10s\")\n        )\n    \n    def discover_service(self, service_name):\n        services = self.consul.health.service(service_name)[1]\n        if services:\n            service = random.choice(services)\n            return f\"http://{service['Service']['Address']}:{service['Service']['Port']}\"\n        return None\n</code></pre>\n<h2>Database per Service Pattern</h2>\n<h3>User Service Database</h3>\n<pre><code class=\"language-python\"># user_service/models.py\nfrom sqlalchemy import Column, Integer, String, DateTime\nfrom sqlalchemy.ext.declarative import declarative_base\n\nBase = declarative_base()\n\nclass User(Base):\n    __tablename__ = \"users\"\n    \n    id = Column(Integer, primary_key=True)\n    email = Column(String, unique=True, index=True)\n    name = Column(String)\n    created_at = Column(DateTime)\n</code></pre>\n<h3>Order Service Database</h3>\n<pre><code class=\"language-python\"># order_service/models.py\nfrom sqlalchemy import Column, Integer, String, DateTime, ForeignKey\nfrom sqlalchemy.ext.declarative import declarative_base\n\nBase = declarative_base()\n\nclass Order(Base):\n    __tablename__ = \"orders\"\n    \n    id = Column(Integer, primary_key=True)\n    user_id = Column(Integer)  # Reference to user in another service\n    product_name = Column(String)\n    amount = Column(Integer)\n    created_at = Column(DateTime)\n</code></pre>\n<h2>API Gateway Implementation</h2>\n<pre><code class=\"language-python\">from fastapi import FastAPI, HTTPException\nfrom fastapi.middleware.cors import CORSMiddleware\nimport httpx\n\napp = FastAPI()\n\napp.add_middleware(\n    CORSMiddleware,\n    allow_origins=[\"*\"],\n    allow_credentials=True,\n    allow_methods=[\"*\"],\n    allow_headers=[\"*\"],\n)\n\nclass APIGateway:\n    def __init__(self):\n        self.services = {\n            \"user\": \"http://user-service:8000\",\n            \"order\": \"http://order-service:8000\",\n            \"payment\": \"http://payment-service:8000\"\n        }\n    \n    async def forward_request(self, service_name: str, path: str, method: str, data: dict = None):\n        service_url = self.services.get(service_name)\n        if not service_url:\n            raise HTTPException(status_code=404, detail=\"Service not found\")\n        \n        async with httpx.AsyncClient() as client:\n            response = await client.request(\n                method=method,\n                url=f\"{service_url}{path}\",\n                json=data\n            )\n            return response.json()\n\ngateway = APIGateway()\n\n@app.get(\"/users/{user_id}\")\nasync def get_user(user_id: int):\n    return await gateway.forward_request(\"user\", f\"/users/{user_id}\", \"GET\")\n\n@app.post(\"/orders\")\nasync def create_order(order_data: dict):\n    return await gateway.forward_request(\"order\", \"/orders\", \"POST\", order_data)\n</code></pre>\n<h2>Monitoring and Observability</h2>\n<h3>Distributed Tracing with OpenTelemetry</h3>\n<pre><code class=\"language-python\">from opentelemetry import trace\nfrom opentelemetry.exporter.jaeger.thrift import JaegerExporter\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import BatchSpanProcessor\n\n# Configure tracing\ntrace.set_tracer_provider(TracerProvider())\ntracer = trace.get_tracer(__name__)\n\njaeger_exporter = JaegerExporter(\n    agent_host_name=\"localhost\",\n    agent_port=14268,\n)\n\nspan_processor = BatchSpanProcessor(jaeger_exporter)\ntrace.get_tracer_provider().add_span_processor(span_processor)\n\n# Use in your services\n@app.get(\"/users/{user_id}\")\nasync def get_user(user_id: int):\n    with tracer.start_as_current_span(\"get_user\") as span:\n        span.set_attribute(\"user.id\", user_id)\n        # Your service logic here\n        return {\"user_id\": user_id}\n</code></pre>\n<h2>Deployment with Docker</h2>\n<h3>Dockerfile for Python Services</h3>\n<pre><code class=\"language-dockerfile\">FROM python:3.11-slim\n\nWORKDIR /app\n\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\nCOPY . .\n\nCMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n</code></pre>\n<h3>Docker Compose for Local Development</h3>\n<pre><code class=\"language-yaml\">version: '3.8'\n\nservices:\n  user-service:\n    build: ./user-service\n    ports:\n      - \"8001:8000\"\n    environment:\n      - DATABASE_URL=postgresql://user:password@db:5432/user_db\n    depends_on:\n      - db\n      - rabbitmq\n\n  order-service:\n    build: ./order-service\n    ports:\n      - \"8002:8000\"\n    environment:\n      - DATABASE_URL=postgresql://user:password@db:5432/order_db\n    depends_on:\n      - db\n      - rabbitmq\n\n  api-gateway:\n    build: ./api-gateway\n    ports:\n      - \"8000:8000\"\n    depends_on:\n      - user-service\n      - order-service\n\n  rabbitmq:\n    image: rabbitmq:3-management\n    ports:\n      - \"5672:5672\"\n      - \"15672:15672\"\n\n  db:\n    image: postgres:13\n    environment:\n      - POSTGRES_PASSWORD=password\n    volumes:\n      - postgres_data:/var/lib/postgresql/data\n\nvolumes:\n  postgres_data:\n</code></pre>\n<h2>Best Practices</h2>\n<ol>\n<li><strong>Service Boundaries</strong>: Define clear service boundaries based on business capabilities</li>\n<li><strong>Data Consistency</strong>: Use eventual consistency and saga patterns for distributed transactions</li>\n<li><strong>Error Handling</strong>: Implement circuit breakers and retry mechanisms</li>\n<li><strong>Security</strong>: Use API keys, JWT tokens, and proper authentication</li>\n<li><strong>Testing</strong>: Write comprehensive tests for each service and integration tests</li>\n<li><strong>Documentation</strong>: Maintain clear API documentation for each service</li>\n</ol>\n<h2>Conclusion</h2>\n<p>Building scalable microservices with Python and FastAPI requires careful planning and implementation. By following the patterns and practices outlined in this guide, you can create a robust, maintainable microservices architecture that can scale with your business needs.</p>\n<p>Remember to start simple and gradually add complexity as your system grows. Monitor your services closely and be prepared to refactor as you learn more about your domain and requirements.</p>\n"
  }
]

// Get featured blog posts (top 3)
export const featuredBlogPosts = allBlogPosts.slice(0, 3)

export default {
  allBlogPosts,
  featuredBlogPosts,
}
